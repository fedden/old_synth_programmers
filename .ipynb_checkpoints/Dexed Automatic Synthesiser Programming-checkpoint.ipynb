{
 "metadata": {
  "name": "",
  "signature": "sha256:508fb944da600f1323a659ea76ede610086d911d3efe52d9f3055984166b2ebe"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First, import the modules relevent to this notebok. We then instanciate\n",
      "the Dexed synthesiser in the VST host RenderMan. We also make a patch generator, capable of simply generating random patches (sets of parameters) for a given synth. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import librenderman as rm\n",
      "import tensorflow as tf\n",
      "from tensorflow.contrib import rnn\n",
      "import numpy as np\n",
      "from sklearn import preprocessing\n",
      "\n",
      "midi_note = 40\n",
      "midi_velocity = 127\n",
      "note_length = 0.8\n",
      "render_length = 2.1\n",
      "\n",
      "engine = rm.RenderEngine(44100, 512, 2048)\n",
      "path = \"/home/tollie/Development/vsts/dexed/Builds/Linux/build/Dexed.so\"\n",
      "\n",
      "if engine.load_plugin(path):\n",
      "    \n",
      "    generator = rm.PatchGenerator(engine)\n",
      "    print \"Successfully loaded Dexed\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Successfully loaded Dexed\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Function for generating a train, test and validation dataset for both\n",
      "# mfccs and rms frames.\n",
      "\n",
      "def generate_data(midi_note,\n",
      "                  midi_velocity,\n",
      "                  note_length,\n",
      "                  render_length,\n",
      "                  engine, \n",
      "                  generator, \n",
      "                  examples_amount):\n",
      "    \n",
      "    examples = []\n",
      "    \n",
      "    for i in range(examples_amount):\n",
      "        \n",
      "        # A random synthesiser preset in the form of a tuple.\n",
      "        random_patch = generator.get_random_patch()\n",
      "        \n",
      "        # Program in the patch on the synth. \n",
      "        engine.set_patch(random_patch)\n",
      "        \n",
      "        # Render the latent audio and features that come with such a\n",
      "        # patch.\n",
      "        engine.render_patch(midi_note,\n",
      "                            midi_velocity,\n",
      "                            note_length,\n",
      "                            render_length)\n",
      "        \n",
      "        # Get RMS and MFCC frames of audio recorded in render_patch().\n",
      "        mfcc_frames = np.array(engine.get_mfcc_frames())\n",
      "        rms_frames = np.array(engine.get_rms_frames())\n",
      "        \n",
      "        # Add this example to the examples list.\n",
      "        examples += [(mfcc_frames, rms_frames, random_patch)]\n",
      "        \n",
      "    return examples"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Function to normalise each dataset and return a unified dataset of\n",
      "# feature vectors repsectively for the test, train and validation sets.\n",
      "\n",
      "def normalise_data(examples, mfcc_normaliser, rms_normaliser):\n",
      "    \n",
      "    # This will hold all of the normalised examples.\n",
      "    normalised_examples = []\n",
      "    \n",
      "    # Loop over all the passed in examples.\n",
      "    for i in range(len(examples)):\n",
      "        \n",
      "        # Unpack the current example.\n",
      "        mfcc_frames, rms_frames, synth_patch = examples[i]\n",
      "        \n",
      "        # Respectively normalise MFCCs and RMS frames.\n",
      "        mfcc_normalised = mfcc_normaliser.transform(mfcc_frames)\n",
      "        rms_normalised = rms_normaliser.transform(rms_frames)\n",
      "        \n",
      "        # Repack the now normalised example as a new tuple.\n",
      "        normalised_examples += [(mfcc_normalised, \n",
      "                                 rms_normalised.reshape(-1, 1), \n",
      "                                 synth_patch)]\n",
      "        \n",
      "    return normalised_examples"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Function to concatenate an examples X data (RMS and MFCC frames) so\n",
      "# they are a joined feature vector.\n",
      "\n",
      "def create_feature_vector_examples(examples):\n",
      "    \n",
      "    feature_vector_examples = []\n",
      "    \n",
      "    # Looping through the passed in examples.\n",
      "    for example in examples:\n",
      "        \n",
      "        mfcc_frames, rms_frames, synth_patch = example\n",
      "        \n",
      "        # Each RMS frame and MFCCs Frame is concatenated into a\n",
      "        # respective feature vector.\n",
      "        feature_vector = np.hstack((mfcc_frames, rms_frames))\n",
      "        \n",
      "        feature_vector_examples += [(feature_vector, synth_patch)]\n",
      "        \n",
      "    return feature_vector_examples"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Fit a sklearn normaliser respectively for both the audio and mfccs\n",
      "# using a newly generated dataset.\n",
      "\n",
      "# Get enough training examples for the sklearn normaliser so that it can\n",
      "# normalise unseen examples well.\n",
      "fitting_amount = 100\n",
      "fitting_data = generate_data(midi_note,\n",
      "                             midi_velocity,\n",
      "                             note_length,\n",
      "                             render_length,\n",
      "                             engine,\n",
      "                             generator,\n",
      "                             fitting_amount)\n",
      "\n",
      "# Take the list of tuples of mfcc and rms arrays and split them into rms\n",
      "# and mfcc arrays.\n",
      "split_data = [np.array(list(t)) for t in zip(*fitting_data)]\n",
      "\n",
      "# Reshape the mfccs so that it is just a 2d stack of mfcc features,\n",
      "# rather than a list of 2d mfcc features for each respective example.\n",
      "fitting_mfccs = split_data[0].reshape(-1, split_data[0].shape[2])\n",
      "fitting_rms = split_data[1]\n",
      "\n",
      "mfcc_normaliser = preprocessing.Normalizer().fit(fitting_mfccs)\n",
      "rms_normaliser = preprocessing.Normalizer().fit(fitting_rms)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def remove_patch_indices(examples):\n",
      "    \n",
      "    no_index_examples = []\n",
      "    \n",
      "    for example in examples:\n",
      "        \n",
      "        _, __, synth_patch = example\n",
      "        no_indices_patch = np.array([p[1] for p in synth_patch])\n",
      "        no_index_examples += [(_, __, no_indices_patch)]\n",
      "        \n",
      "    return no_index_examples\n",
      "\n",
      "def generate_examples(amount,\n",
      "                      midi_note,\n",
      "                      midi_velocity,\n",
      "                      note_length,\n",
      "                      render_length,\n",
      "                      engine,\n",
      "                      generator,\n",
      "                      mfcc_normaliser,\n",
      "                      rms_normaliser):\n",
      "    \n",
      "    examples = generate_data(midi_note,\n",
      "                             midi_velocity,\n",
      "                             note_length,\n",
      "                             render_length,\n",
      "                             engine,\n",
      "                             generator,\n",
      "                             1)\n",
      "    \n",
      "    examples = remove_patch_indices(examples)\n",
      "    \n",
      "    normalised_examples = normalise_data(examples,\n",
      "                                         mfcc_normaliser,\n",
      "                                         rms_normaliser)\n",
      "    \n",
      "    return create_feature_vector_examples(normalised_examples)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Turns an array of parameter settings into an array of normalised\n",
      "# feature vectors for MSE error checking, here is the method:\n",
      "#\n",
      "# 1) Turn array of values into list of tuples.\n",
      "# 2) Load patch into engine and render it.\n",
      "# 3) Get MFCCs and RMS frame.\n",
      "# 4) Normalise respecive frames.\n",
      "# 5) Horizontally stack mfccs and rms frames into feature vector.\n",
      "# 6) Concatenate all feature vectors into a list.\n",
      "\n",
      "def get_features(patches, \n",
      "                 midi_note,\n",
      "                 midi_velocity,\n",
      "                 note_length,\n",
      "                 render_length):\n",
      "    global engine\n",
      "    global generator\n",
      "    global mfcc_normaliser\n",
      "    global rms_normaliser\n",
      "    \n",
      "    features = []\n",
      "    \n",
      "    for patch in patches:\n",
      "        \n",
      "        patch_tuple = []\n",
      "        patch = patch.reshape(-1, 1)\n",
      "\n",
      "        for i in range(len(patch)):\n",
      "            patch_tuple += [(i, patch[i])]\n",
      "            \n",
      "        print patch_tuple\n",
      "        engine.set_patch(patch_tuple)\n",
      "        engine.render_patch(midi_note,\n",
      "                            midi_velocity,\n",
      "                            note_length,\n",
      "                            render_length)\n",
      "        \n",
      "        mfccs = engine.get_mfcc_frames()\n",
      "        rms = engine.get_rms_frames()\n",
      "        \n",
      "        mfccs_normal = mfcc_normaliser.transform(mfccs)\n",
      "        rms_normal = rms_normaliser.transform(rms)\n",
      "        \n",
      "        features += [np.hstack((mfccs_normal, rms_normal))]\n",
      "    \n",
      "    return np.array(features)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 47
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Create the tensorflow graph.\n",
      "dimension_data_example = generate_examples(1,\n",
      "                                           midi_note,\n",
      "                                           midi_velocity,\n",
      "                                           note_length,\n",
      "                                           render_length,\n",
      "                                           engine,\n",
      "                                           generator,\n",
      "                                           mfcc_normaliser,\n",
      "                                           rms_normaliser)\n",
      "\n",
      "features, parameters = dimension_data_example[0]\n",
      "# https://github.com/aymericdamien/TensorFlow-Examples/blob/master/\n",
      "# notebooks/3_NeuralNetworks/recurrent_network.ipynb\n",
      "\n",
      "# Parameters for the tensorflow graph.\n",
      "learning_rate = 0.001\n",
      "training_iters = 256\n",
      "batch_size = 128\n",
      "display_step = 10\n",
      "number_hidden = 128  \n",
      "\n",
      "# Network parameters:\n",
      "# 14 - amount of mfccs + rms value\n",
      "number_input = int(features.shape[1]) \n",
      "\n",
      "# 181 - amount of samples per example\n",
      "number_timesteps = int(features.shape[0])\n",
      "\n",
      "# 155 - amount of parameters\n",
      "number_outputs = len(parameters)     "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  DeprecationWarning)\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x1 = tf.placeholder(\"float\", [None, number_timesteps, number_input])\n",
      "x2 = tf.placeholder(\"float\", [None, number_timesteps, number_input])\n",
      "x_from_y = tf.placeholder(\"float\", [None, number_timesteps, number_input])\n",
      "#y = tf.placeholder(\"float\", [None, number_outputs])\n",
      "\n",
      "weights = {\n",
      "    \"out\": tf.Variable(tf.random_normal([number_hidden, number_outputs]))\n",
      "}\n",
      "\n",
      "biases = {\n",
      "    \"out\": tf.Variable(tf.random_normal([number_outputs]))\n",
      "}\n",
      "\n",
      "def RNN(x, weights, biases):\n",
      "    \n",
      "    # Prepare data shape to match 'rnn' function requirenents\n",
      "    x = tf.transpose(x, [1, 0, 2])\n",
      "    x = tf.reshape(x, [-1, number_input])\n",
      "    x = tf.split(0, number_timesteps, x)\n",
      "    tf.get_variable_scope().reuse_variables()\n",
      "    lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(number_hidden, forget_bias=1.0)\n",
      "    outputs, states = tf.nn.rnn(lstm_cell, x, dtype=tf.float32)\n",
      "    \n",
      "    return tf.matmul(outputs[-1], weights['out']) + biases['out']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "prediction = RNN(x1, weights, biases)\n",
      "\n",
      "cost = tf.sqrt(tf.reduce_mean(tf.square(tf.sub(x2, x_from_y))))\n",
      "optimiser = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ValueError",
       "evalue": "No gradients provided for any variable, check your graph for ops that do not support gradients, between variables ['Tensor(\"Variable/read:0\", shape=(128, 155), dtype=float32)', 'Tensor(\"Variable_1/read:0\", shape=(155,), dtype=float32)', 'Tensor(\"RNN/BasicLSTMCell/Linear/Matrix/read:0\", shape=(142, 512), dtype=float32)', 'Tensor(\"RNN/BasicLSTMCell/Linear/Bias/read:0\", shape=(512,), dtype=float32)', 'Tensor(\"Variable_2/read:0\", shape=(128, 155), dtype=float32)', 'Tensor(\"Variable_3/read:0\", shape=(155,), dtype=float32)'] and loss Tensor(\"Sqrt_3:0\", shape=(), dtype=float32).",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-15-9cc8d1553324>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_from_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0moptimiser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.pyc\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\u001b[0m\n\u001b[1;32m    274\u001b[0m           \u001b[0;34m\"No gradients provided for any variable, check your graph for ops\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m           \u001b[0;34m\" that do not support gradients, between variables %s and loss %s.\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m           ([str(v) for _, v in grads_and_vars], loss))\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     return self.apply_gradients(grads_and_vars, global_step=global_step,\n",
        "\u001b[0;31mValueError\u001b[0m: No gradients provided for any variable, check your graph for ops that do not support gradients, between variables ['Tensor(\"Variable/read:0\", shape=(128, 155), dtype=float32)', 'Tensor(\"Variable_1/read:0\", shape=(155,), dtype=float32)', 'Tensor(\"RNN/BasicLSTMCell/Linear/Matrix/read:0\", shape=(142, 512), dtype=float32)', 'Tensor(\"RNN/BasicLSTMCell/Linear/Bias/read:0\", shape=(512,), dtype=float32)', 'Tensor(\"Variable_2/read:0\", shape=(128, 155), dtype=float32)', 'Tensor(\"Variable_3/read:0\", shape=(155,), dtype=float32)'] and loss Tensor(\"Sqrt_3:0\", shape=(), dtype=float32)."
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Initializing the variables\n",
      "init = tf.global_variables_initializer()\n",
      "\n",
      "# Launching the graph\n",
      "with tf.Session() as sess:\n",
      "    \n",
      "    sess.run(init)\n",
      "    step = 1\n",
      "    \n",
      "    while step * batch_size < training_iters:\n",
      "        \n",
      "        train_batch = generate_examples(batch_size,\n",
      "                                        midi_note,\n",
      "                                        midi_velocity,\n",
      "                                        note_length,\n",
      "                                        render_length,\n",
      "                                        engine,\n",
      "                                        generator,\n",
      "                                        mfcc_normaliser,\n",
      "                                        rms_normaliser)\n",
      "        \n",
      "        split_train = map(list, zip(*train_batch))\n",
      "        batch_x = split_train[0]\n",
      "        \n",
      "        pred = sess.run([prediction], feed_dict={x1: batch_x})\n",
      "        \n",
      "        features_from_prediction = get_features(pred,\n",
      "                                                midi_note,\n",
      "                                                midi_velocity,\n",
      "                                                note_length,\n",
      "                                                render_length)\n",
      "        \n",
      "        sess.run(optimiser, feed_dict={x2: batch_x,\n",
      "                                       x_from_y: features_from_prediction})\n",
      "        \n",
      "        if step % display_step == 0:\n",
      "            \n",
      "            loss = sess.run(cost, feed_dict={x2: batch_x,\n",
      "                                             x_from_y: features_from_prediction})\n",
      "            print \"{:5d}\".format(step * batch_size) + \": batch loss= \" + \\\n",
      "                  \"{:.6f}\".format(loss)\n",
      "        \n",
      "        step += 1\n",
      "        \n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  DeprecationWarning)\n"
       ]
      },
      {
       "ename": "ValueError",
       "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-48-a7d4e62d55d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m                                         \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                                         \u001b[0mmfcc_normaliser\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                                         rms_normaliser)\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0msplit_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-6-89fcf6858a74>\u001b[0m in \u001b[0;36mgenerate_examples\u001b[0;34m(amount, midi_note, midi_velocity, note_length, render_length, engine, generator, mfcc_normaliser, rms_normaliser)\u001b[0m\n\u001b[1;32m     33\u001b[0m     normalised_examples = normalise_data(examples,\n\u001b[1;32m     34\u001b[0m                                          \u001b[0mmfcc_normaliser\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                                          rms_normaliser)\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcreate_feature_vector_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalised_examples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-3-d97e35697e32>\u001b[0m in \u001b[0;36mnormalise_data\u001b[0;34m(examples, mfcc_normaliser, rms_normaliser)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# Respectively normalise MFCCs and RMS frames.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mmfcc_normalised\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmfcc_normaliser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmfcc_frames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mrms_normalised\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrms_normaliser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrms_frames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# Repack the now normalised example as a new tuple.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/data.pyc\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X, y, copy)\u001b[0m\n\u001b[1;32m   1437\u001b[0m         \"\"\"\n\u001b[1;32m   1438\u001b[0m         \u001b[0mcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    405\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[1;32m    406\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     56\u001b[0m             and not np.isfinite(X).all()):\n\u001b[1;32m     57\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[0;32m---> 58\u001b[0;31m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
       ]
      }
     ],
     "prompt_number": 48
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}